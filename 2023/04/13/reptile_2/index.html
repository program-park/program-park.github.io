<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Python 爬虫之如何对爬取到的数据进行解析 | 程序园</title><meta name="author" content="大Null"><meta name="copyright" content="大Null"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="以 Python 为基础的爬虫教程，教你如何使用 Xpath、JsonPath、BeautifulSoup 三款工具对数据进行解析。">
<meta property="og:type" content="article">
<meta property="og:title" content="Python 爬虫之如何对爬取到的数据进行解析">
<meta property="og:url" content="https://www.program-park.top/2023/04/13/reptile_2/index.html">
<meta property="og:site_name" content="程序园">
<meta property="og:description" content="以 Python 为基础的爬虫教程，教你如何使用 Xpath、JsonPath、BeautifulSoup 三款工具对数据进行解析。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.program-park.top/img/reptile/1.png">
<meta property="article:published_time" content="2023-04-13T09:00:00.000Z">
<meta property="article:modified_time" content="2023-04-13T09:22:40.162Z">
<meta property="article:author" content="大Null">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.program-park.top/img/reptile/1.png"><link rel="shortcut icon" href="/img/favicon_logo/favicon.png"><link rel="canonical" href="https://www.program-park.top/2023/04/13/reptile_2/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":350},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":30,"languages":{"author":"作者: 大Null","link":"链接: ","source":"来源: 程序园","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: false,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Python 爬虫之如何对爬取到的数据进行解析',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-04-13 17:22:40'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><script>window.paceOptions = {
  restartOnPushState: false
}

document.addEventListener('pjax:send', () => {
  Pace.restart()
})
</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatat_img.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">206</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">37</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="程序园"><span class="site-name">程序园</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Python 爬虫之如何对爬取到的数据进行解析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2023-04-13T09:00:00.000Z" title="发表于 2023-04-13 17:00:00">2023-04-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>13分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Python 爬虫之如何对爬取到的数据进行解析"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p><strong>本文章中所有内容仅供学习交流使用，不用于其他任何目的，严禁用于商业用途和非法用途，否则由此产生的一切后果均与作者无关。</strong></p>
</blockquote>
<h1 id="1-前言">1. 前言</h1>
<p>  在上一篇博客中，讲了如何使用 urllib 库爬取网页的数据，但是根据博客流程去操作的人应该能发现，我们爬取到的数据是整个网页返回的源码，到手的数据对我们来说是又乱又多的，让我们不能快速、准确的定位到所需数据，所以，这一篇就来讲讲如何对爬取的数据进行解析，拿到我们想要的部分。<br>
  下面会依次讲解目前市场上用的比较多的解析数据的三种方式：<code>Xpath</code>、<code>JsonPath</code>、<code>BeautifulSoup</code>，以及这三种方式的区别。</p>
<h1 id="2-Xpath">2. Xpath</h1>
<p>  XPath（XML Path Language - XML路径语言），它是一种用来确定 XML 文档中某部分位置的语言，以 XML 为基础，提供用户在数据结构树中寻找节点的能力，Xpath 被很多开发者亲切的称为小型查询语言。</p>
<h2 id="2-1-插件-库安装">2.1 插件/库安装</h2>
<p>  首先，我们要安装 Xpath Helper 插件，借助 Xpath Helper 插件可以帮助我们快速准确的定位并获取 Xpath 路径，解决无法正常定位 Xpath 路径的问题，链接如下：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1tXW9ZtFDDiH5ZCIEpx2deQ">https://pan.baidu.com/s/1tXW9ZtFDDiH5ZCIEpx2deQ</a>，提取码：6666 。<br>
  安装教程如下：</p>
<ol>
<li>打开 Chrome 浏览器，点击右上角小圆点 → 更多工具 → 扩展程序；<br>
<img src="https://img-blog.csdnimg.cn/27e2e91aeed84bc99686f55f6abfcc48.png" alt=""></li>
<li>拖拽 Xpath 插件到扩展程序中（需开启开发者模式）；<br>
<img src="/img/reptile/reptile_2/1.png" alt=""></li>
<li>关闭浏览器重新打开，打开<code>www.baidu.com</code>，使用快捷键<code>ctrl + shift + x</code>，出现小黑框即代表安装完毕。</li>
</ol>
<p>  安装 Xpath Helper 插件后，我们还需要在本地的 Python 环境上安装 <code>lxml</code> 库，命令如下：<code>pip3 install lxml</code>。</p>
<p><img src="/img/reptile/reptile_2/2.png" alt=""></p>
<h2 id="2-2-基础使用">2.2 基础使用</h2>
<p>  首先，我们打开<code>Pycharm</code>，创建新脚本，实例化一个<code>etree</code>的对象，并将被解析的页面源码数据加载到该对象中。在用 Xpath 解析文件时，会有两种情况，一种是将本地的文档源码数据加载到<code>etree</code>中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析本地文件</span></span><br><span class="line">tree = etree.parse(<span class="string">&#x27;XXX.html&#x27;</span>)</span><br><span class="line">tree.xpath(<span class="string">&#x27;Xpath表达式&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>  另一种是将从互联网上获取的源码数据加载到<code>etree</code>中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析互联网页面源码数据</span></span><br><span class="line">tree = etree.HTML(response.read().decode(<span class="string">&#x27;utf‐8&#x27;</span>))</span><br><span class="line">tree.xpath(<span class="string">&#x27;Xpath表达式&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>  使用 Xpath 解析数据，最重要的便是 Xpath 表达式的书写，对 Xpath 表达式的熟悉程度将直接影响到数据解析的效率和精确度。</p>
<h2 id="2-3-Xpath表达式">2.3 Xpath表达式</h2>
<table>
	<tr>
		<th align=center>表达式</th>
		<th>描述</th>
	</tr>
		<tr>
		<td align=center>/ </td>
		<td>表示的是从根节点开始定位，表示的是一个层级</td>
	</tr>
	</tr>
		<tr>
		<td align=center>//</td>
		<td>表示的是多个层级，可以表示从任意位置开始定位</td>
	</tr>
	</tr>
		<tr>
		<td align=center>.</td>
		<td>选取当前节点</td>
	</tr>
	</tr>
		<tr>
		<td align=center>..</td>
		<td>选取当前节点的父节点</td>
	</tr>
	</tr>
		<tr>
		<td align=center>@</td>
		<td>选取属性</td>
	</tr>
	</tr>
		<tr>
		<td align=center>*</td>
		<td>通配符，选择所有元素节点与元素名</td>
	</tr>
	</tr>
		<tr>
		<td align=center>@*</td>
		<td>选取所有属性</td>
	</tr>
	</tr>
		<tr>
		<td align=center>[@attrib]</td>
		<td>选取具有给定属性的所有元素</td>
	</tr>
	</tr>
		<tr>
		<td align=center>[@attrib=‘value’]</td>
		<td>选取给定属性具有给定值的所有元素</td>
	</tr>
	</tr>
		<tr>
		<td align=center>[tag]</td>
		<td>选取所有具有指定元素的直接子节点</td>
	</tr>
	</tr>
		<tr>
		<td align=center>[tag=‘text’]</td>
		<td>选取所有具有指定元素并且文本内容是 text 节点</td>
	</tr>
</table>
<p>  <strong>示例：</strong></p>
<table>
	<tr>
		<th align=center>路径表达式</th>
		<th>描述</th>
	</tr>
	<tr>
		<td align=center>bookstore</td>
		<td>选取 bookstore 元素的所有子节点</td>
	</tr>
	<tr>
		<td align=center>/bookstore</td>
		<td>选取根元素 bookstore（加入路径起始于 /，则此路径始终代表到某元素的绝对路径）</td>
	</tr>
	<tr>
		<td align=center>bookstore/book</td>
		<td>选取属于 bookstore 的子元素的所有 book 元素</td>
	</tr>
	<tr>
		<td align=center>bookstore/book[1]</td>
		<td>选取属于 bookstore 的子元素的第一个 book 元素</td>
	</tr>
	<tr>
		<td align=center>bookstore/book[last()]</td>
		<td>选取属于 bookstore 的子元素的最后一个 book 元素</td>
	</tr>
	<tr>
		<td align=center>bookstore/book[last()-1]</td>
		<td>选取属于 bookstore 的子元素的倒数第二个 book 元素</td>
	</tr>
	<tr>
		<td align=center>bookstore/book[position()<3]</td>
		<td>选取属于 bookstore 的子元素的前两个 book 元素</td>
	</tr>
	<tr>
		<td align=center>//book</td>
		<td>选取所有 book 子元素，而不管它们在文档中的位置</td>
	</tr>
	<tr>
		<td align=center>bookstore//book</td>
		<td>选择属于 bookstore 元素的后代的素有 book 元素，而不管它们位于 bookstore 之下的什么位置</td>
	</tr>
	<tr>
		<td align=center>//@lang</td>
		<td>选取名为 lang 的所有属性</td>
	</tr>
	<tr>
		<td align=center>//title[@lang]</td>
		<td>选取所有拥有名为 lang 的属性的 title 元素</td>
	</tr>
	<tr>
		<td align=center>//title[@lang='eng']</td>
		<td>选取所有title元素，且这些元素拥有值为 eng 的 lang 属性</td>
	</tr>
	<tr>
		<td align=center>/bookstore/book[price>35.00]</td>
		<td>选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00</td>
	</tr>
	<tr>
		<td align=center>/bookstore/book[price>35.00]/title</td>
		<td>选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00</td>
	</tr>
	<tr>
		<td align=center>//book/title | //book/price</td>
		<td>选取 book 元素的所有 title 或 price 元素</td>
	</tr>
	<tr>
		<td align=center>child::book</td>
		<td>选取所有属于当前节点的子元素的 book 节点</td>
	</tr>
	<tr>
		<td align=center>attribute::lang</td>
		<td>选取当前节点的 lang 属性</td>
	</tr>
	<tr>
		<td align=center>child::*</td>
		<td>选取当前节点的所有子元素</td>
	</tr>
	<tr>
		<td align=center>attribute::*</td>
		<td>选取当前节点的属性</td>
	</tr>
	<tr>
		<td align=center>child::text()</td>
		<td>选取当前节点的所有文本子节点</td>
	</tr>
	<tr>
		<td align=center>child::node()</td>
		<td>选取当前节点的所有子节点</td>
	</tr>
	<tr>
		<td align=center>descendant::book</td>
		<td>选取当前节点的所哟 book 后代</td>
	</tr>
	<tr>
		<td align=center>ancestor::book</td>
		<td>选择当前节点的所有 book 先辈</td>
	</tr>
	<tr>
		<td align=center>ancestor-or-self::book</td>
		<td>选取当前节点的所有 book 先辈以及当前节点（如果此节点是 book 节点）</td>
	</tr>
	<tr>
		<td align=center>child::*/child::price</td>
		<td>选取当前节点的所有 price 孙节点</td>
	</tr>
	<tr>
		<td align=center>//li[contains(@id,"h")]</td>
		<td>选取 id 属性中包含 h 的 li 标签（模糊查询）</td>
	</tr>
	<tr>
		<td align=center>//li[starts-with(@id,"h")]</td>
		<td>选取 id 属性中以 h 为开头的 li 标签</td>
	</tr>
	<tr>
		<td align=center>//li[@id="h1" and @class="h2"]</td>
		<td>选取 id 属性为 h1 且 class 属性为 h2 的 li 标签</td>
	</tr>
</table>
<h2 id="2-4-案例演示">2.4 案例演示</h2>
<h3 id="2-4-1-某度网站案例">2.4.1 某度网站案例</h3>
<p>  <strong>需求：</strong> 获取<code>某度一下</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.某du.com/&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span> : <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 请求对象的定制</span></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line"><span class="comment"># 模拟浏览器访问服务器</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"><span class="comment"># 获取网页源码</span></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析服务器响应的文件</span></span><br><span class="line">tree = etree.HTML(content)</span><br><span class="line"><span class="comment"># 获取想要的路径</span></span><br><span class="line">result = tree.xpath(<span class="string">&#x27;//input[@id=&quot;su&quot;]/@value&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<h1 id="3-JsonPath">3. JsonPath</h1>
<p>  JsonPath 是一种信息抽取类库，是从 JSON 文档中抽取指定信息的工具，提供多种语言实现版本，包括：Javascript、Python、PHP 和 Java。<br>
  JsonPath 对于 JSON 来说，相当于 Xpath 对于 XML。<br>
  官方文档：<a target="_blank" rel="noopener" href="http://goessner.net/articles/JsonPath">http://goessner.net/articles/JsonPath</a></p>
<h2 id="3-1-库安装">3.1 库安装</h2>
<p>  在本地的 Python 环境上安装 JsonPath 库，命令如下：<code>pip3 install jsonpath</code>。</p>
<h2 id="3-2-基础使用">3.2 基础使用</h2>
<p>  JsonPath 和 Xpath 的区别在于，JsonPath 只能对本地文件进行操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> jsonpath</span><br><span class="line"></span><br><span class="line">obj = json.load(<span class="built_in">open</span>(<span class="string">&#x27;json文件&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf‐8&#x27;</span>))</span><br><span class="line">ret = jsonpath.jsonpath(obj, <span class="string">&#x27;jsonpath语法&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="3-2-JsonPath表达式">3.2 JsonPath表达式</h2>
<table>
	<tr>
		<th>JsonPath</th>
		<th>释义</th>
	</tr>
	<tr>
		<td align=center>$</td>
		<td>根节点/元素</td>
	</tr>
	<tr>
		<td align=center>.  或 []</td>
		<td>子元素</td>
	</tr>
	<tr>
		<td align=center>..</td>
		<td>递归下降（从 E4X 借用了这个语法）</td>
	</tr>
	<tr>
		<td align=center>@</td>
		<td>当前节点/元素</td>
	</tr>
	<tr>
		<td align=center>?()</td>
		<td>应用过滤表达式，一般需要结合 [?(@  )] 来使用</td>
	</tr>
	<tr>
		<td align=center>[]</td>
		<td>子元素操作符，（可以在里面做简单的迭代操作，如数据索引，根据内容选值等）</td>
	</tr>
	<tr>
		<td align=center>[,]</td>
		<td>支持迭代器中做多选，多个 key 用逗号隔开</td>
	</tr>
	<tr>
		<td align=center>[start：end：step]</td>
		<td>数组分割操作，等同于切片</td>
	</tr>
	<tr>
		<td align=center>()</td>
		<td>脚本表达式，使用在脚本引擎下面</td>
	</tr>
</table>
<p>  更多表达式用法可查看官方文档：<a target="_blank" rel="noopener" href="http://goessner.net/articles/JsonPath">http://goessner.net/articles/JsonPath</a>。</p>
<h2 id="3-3-案例演示">3.3 案例演示</h2>
<p>  <strong>需求：</strong> 获取<code>淘票票</code>数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> jsonpath</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;cookie&#x27;</span>: <span class="string">&#x27;miid=536765677517889060; t=78466542de5dbe84715c098fa2366f87; cookie2=11c90be2b7bda713126ed897ab23e35d; v=0; _tb_token_=ee5863e335344; cna=jYeFGkfrFXoCAXPrFThalDwd; xlly_s=1; tfstk=cdlVBIX7qIdVC-V6pSNwCDgVlVEAa8mxXMa3nx9gjUzPOZeuYsAcXzbAiJwAzG2c.; l=eBxbMUncLj6r4x9hBO5aourza77T6BAb4sPzaNbMiInca6BOT3r6QNCnaDoy7dtjgtCxretPp0kihRLHR3xg5c0c07kqm0JExxvO.; isg=BHBwrClf5nUOJrpxMvRIOGsqQT7CuVQDlydQ-WrHREsaJRDPEsmVk5EbfS1FtQzb&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;referer&#x27;</span>: <span class="string">&#x27;https://dianying.taobao.com/&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;content-type&#x27;</span>: <span class="string">&#x27;text/html;charset=UTF-8&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_request</span>():</span><br><span class="line">    res_obj = urllib.request.Request(url=<span class="string">&quot;https://dianying.taobao.com/cityAction.json?activityId&amp;_ksTS=1644570795658_173&amp;jsoncallback=jsonp174&amp;action=cityAction&amp;n_s=new&amp;event_submit_doGetAllRegion=true&quot;</span>,headers=headers)</span><br><span class="line">    <span class="keyword">return</span> res_obj</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_context</span>(<span class="params">req_obj</span>):</span><br><span class="line">    resp = urllib.request.urlopen(req_obj)</span><br><span class="line">    origin_context = resp.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    result = origin_context.split(<span class="string">&#x27;jsonp174(&#x27;</span>)[<span class="number">1</span>].split(<span class="string">&#x27;)&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_and_parse</span>(<span class="params">context</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;jsonpath_淘票票案例.json&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(context)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_json</span>():</span><br><span class="line">    obj = json.load(<span class="built_in">open</span>(<span class="string">&#x27;jsonpath_淘票票案例.json&#x27;</span>, mode=<span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">    region_name_list = jsonpath.jsonpath(obj, <span class="string">&#x27;$..regionName&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(region_name_list)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(region_name_list))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    req_obj = create_request()</span><br><span class="line">    context = get_context(req_obj)</span><br><span class="line">    download_and_parse(context)</span><br><span class="line">    parse_json()</span><br></pre></td></tr></table></figure>
<h1 id="4-BeautifulSoup">4. BeautifulSoup</h1>
<p>  BeautifulSoup 是 Python 的一个 HTML 的解析库，我们常称之为 bs4，可以通过它来实现对网页的解析，从而获得想要的数据。<br>
  在用 BeautifulSoup 库进行网页解析时，还是要依赖解析器，BeautifulSoup 支持 Python 标准库中的 HTML 解析器，除此之外，还支持一些第三方的解析器，如果我们不安装第三方解析器，则会试用 Python 默认的解析器，而在第三方解析器中，我推荐试用 lxml，它的解析速度快、容错能力比较强。</p>
<table>
	<tr>
		<th>解析器</th>
		<th>使用方法</th>
		<th>优势</th>
	</tr>
	<tr>
		<td align=center>Python 标准库 </td>
		<td>BeautifulSoup(markup, “html.parser”)</td>
		<td>Python 的内置标准库、执行速度适中、文档容错能力强</td>
	</tr>
	<tr>
		<td align=center>lxml HTML 解析器</td>
		<td>BeautifulSoup(markup, “lxml”)</td>
		<td>速度快、文档容错能力强</td>
	</tr>
	<tr>
		<td align=center>lxml XML 解析器</td>
		<td>BeautifulSoup(markup, [“lxml”, “xml”]) 或 BeautifulSoup(markup, “xml”)</td>
		<td>速度快、唯一支持 XML 的解析器</td>
	</tr>
	<tr>
		<td align=center>html5lib</td>
		<td> BeautifulSoup(markup, “html5lib”)</td>
		<td>最好的容错性、以浏览器的方式解析文档、生成 HTML5 格式的文档、不依赖外部扩展</td>
	</tr>
</table>
<h2 id="4-1-库安装">4.1 库安装</h2>
<p>  在本地的 Python 环境上安装 BeautifulSoup 库，命令如下：<code>pip3 install bs4</code>。</p>
<h2 id="4-2-基础使用">4.2 基础使用</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认打开文件的编码格式是gbk，所以需要指定打开编码格式</span></span><br><span class="line"><span class="comment"># 服务器响应的文件生成对象</span></span><br><span class="line"><span class="comment"># soup = BeautifulSoup(response.read().decode(), &#x27;lxml&#x27;)</span></span><br><span class="line"><span class="comment"># 本地文件生成对象</span></span><br><span class="line">soup = BeautifulSoup(<span class="built_in">open</span>(<span class="string">&#x27;1.html&#x27;</span>), <span class="string">&#x27;lxml&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>  <strong>BeautifulSoup类基本元素：</strong></p>
<table>
<thead>
<tr>
<th style="text-align:center">基本元素</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Tag</td>
<td>标签，最基本的信息组织单元，分别用&lt;&gt;和&lt;/&gt;标明开头和结尾</td>
</tr>
<tr>
<td style="text-align:center">Name</td>
<td>标签的名字，&lt;p&gt;…&lt;/p&gt; 的名字是 ’p’，格式：&lt;tag&gt;.name</td>
</tr>
<tr>
<td style="text-align:center">Attributes</td>
<td>标签的属性，字典组织形式，格式：&lt;tag&gt;.attrs</td>
</tr>
<tr>
<td style="text-align:center">NavigableString</td>
<td>标签内非属性字符串，&lt;&gt;…&lt;/&gt;中字符串，格式：&lt;tag&gt;.string</td>
</tr>
<tr>
<td style="text-align:center">Comment</td>
<td>标签内字符串的注释部分，一种特殊的 Comment 类型</td>
</tr>
</tbody>
</table>
<h2 id="4-3-常见方法">4.3 常见方法</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">soup.title	<span class="comment"># 获取html的title标签的信息</span></span><br><span class="line">soup.a		<span class="comment"># 获取html的a标签的信息(soup.a默认获取第一个a标签，想获取全部就用for循环去遍历)</span></span><br><span class="line">soup.a.name	<span class="comment"># 获取a标签的名字</span></span><br><span class="line">soup.a.parent.name	<span class="comment"># a标签的父标签(上一级标签)的名字</span></span><br><span class="line">soup.a.parent.parent.name	<span class="comment"># a标签的父标签的父标签的名字</span></span><br><span class="line"><span class="built_in">type</span>(soup.a)	<span class="comment"># 查看a标签的类型</span></span><br><span class="line">soup.a.attrs	<span class="comment"># 获取a标签的所有属性(注意到格式是字典)</span></span><br><span class="line"><span class="built_in">type</span>(soup.a.attrs)	<span class="comment"># 查看a标签属性的类型</span></span><br><span class="line">soup.a.attrs[<span class="string">&#x27;class&#x27;</span>]	<span class="comment"># 因为是字典，通过字典的方式获取a标签的class属性</span></span><br><span class="line">soup.a.attrs[<span class="string">&#x27;href&#x27;</span>]	<span class="comment"># 同样，通过字典的方式获取a标签的href属性</span></span><br><span class="line">soup.a.string		<span class="comment"># a标签的非属性字符串信息，表示尖括号之间的那部分字符串</span></span><br><span class="line"><span class="built_in">type</span>(soup.a.string)	<span class="comment"># 查看标签string字符串的类型</span></span><br><span class="line">soup.p.string		<span class="comment"># p标签的字符串信息(注意p标签中还有个b标签，但是打印string时并未打印b标签，说明string类型是可跨越多个标签层次)</span></span><br><span class="line">soup.find_all(<span class="string">&#x27;a&#x27;</span>)	<span class="comment"># 使用find_all()方法通过标签名称查找a标签,返回的是一个列表类型</span></span><br><span class="line">soup.find_all([<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>])	<span class="comment"># 把a标签和b标签作为一个列表传递，可以一次找到a标签和b标签</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> soup.find_all(<span class="string">&#x27;a&#x27;</span>):  <span class="comment"># for循环遍历所有a标签，并把返回列表中的内容赋给t</span></span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&#x27;t的值是：&#x27;</span>, t)  <span class="comment"># link得到的是标签对象</span></span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&#x27;t的类型是：&#x27;</span>, <span class="built_in">type</span>(t))</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&#x27;a标签中的href属性是：&#x27;</span>, t.get(<span class="string">&#x27;href&#x27;</span>))  <span class="comment"># 获取a标签中的url链接</span></span><br><span class="line">      </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> soup.find_all(<span class="literal">True</span>):  <span class="comment"># 如果给出的标签名称是True，则找到所有标签</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;标签名称：&#x27;</span>, i.name)  <span class="comment"># 打印标签名称</span></span><br><span class="line"></span><br><span class="line">soup.find_all(<span class="string">&#x27;a&#x27;</span>, href=<span class="string">&#x27;http://www.xxx.com&#x27;</span>)	<span class="comment"># 标注属性检索</span></span><br><span class="line">soup.find_all(class_=<span class="string">&#x27;title&#x27;</span>)	<span class="comment"># 指定属性，查找class属性为title的标签元素，注意因为class是python的关键字，所以这里需要加个下划线&#x27;_&#x27;</span></span><br><span class="line">soup.find_all(<span class="built_in">id</span>=<span class="string">&#x27;link1&#x27;</span>)	<span class="comment"># 查找id属性为link1的标签元素</span></span><br><span class="line">soup.head	<span class="comment"># head标签</span></span><br><span class="line">soup.head.contents	<span class="comment"># head标签的儿子标签，contents返回的是列表类型</span></span><br><span class="line">soup.body.contents	<span class="comment"># body标签的儿子标签</span></span><br><span class="line"><span class="built_in">len</span>(soup.body.contents)	<span class="comment"># 获得body标签儿子节点的数量</span></span><br><span class="line">soup.body.contents[<span class="number">1</span>]	<span class="comment"># 通过列表索引获取第一个节点的内容</span></span><br><span class="line"><span class="built_in">type</span>(soup.body.children)	<span class="comment"># children返回的是一个迭代对象，只能通过for循环来使用，不能直接通过索引来读取其中的内容</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> soup.body.children:   <span class="comment"># 通过for循环遍历body标签的儿子节点</span></span><br><span class="line">    <span class="built_in">print</span>(i.name)   <span class="comment"># 打印节点的名字</span></span><br></pre></td></tr></table></figure>
<h2 id="4-4-案例演示">4.4 案例演示</h2>
<p>  <strong>需求：</strong> 获取<code>星巴克</code>数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.starbucks.com.cn/menu/&#x27;</span></span><br><span class="line"></span><br><span class="line">resp = urllib.request.urlopen(url)</span><br><span class="line">context = resp.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">soup = BeautifulSoup(context,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">obj = soup.select(<span class="string">&quot;ul[class=&#x27;grid padded-3 product&#x27;] div[class=&#x27;preview circle&#x27;]&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> obj:</span><br><span class="line">    completePicUrl = <span class="string">&#x27;https://www.starbucks.com.cn&#x27;</span>+item.attrs.get(<span class="string">&#x27;style&#x27;</span>).split(<span class="string">&#x27;url(&quot;&#x27;</span>)[<span class="number">1</span>].split(<span class="string">&#x27;&quot;)&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="built_in">print</span>(completePicUrl)</span><br></pre></td></tr></table></figure>
<h1 id="参考文献">参考文献</h1>
<p>  【1】<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_54528857/article/details/122202572">https://blog.csdn.net/qq_54528857/article/details/122202572</a><br>
  【2】<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_46092061/article/details/119777935">https://blog.csdn.net/qq_46092061/article/details/119777935</a><br>
  【3】<a target="_blank" rel="noopener" href="https://www.php.cn/python-tutorials-490500.html">https://www.php.cn/python-tutorials-490500.html</a><br>
  【4】<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/313277380">https://zhuanlan.zhihu.com/p/313277380</a><br>
  【5】<a target="_blank" rel="noopener" href="https://blog.csdn.net/xiaobai729/article/details/124079260">https://blog.csdn.net/xiaobai729/article/details/124079260</a><br>
  【6】<a target="_blank" rel="noopener" href="https://blog.51cto.com/u_15309652/3154785">https://blog.51cto.com/u_15309652/3154785</a><br>
  【7】<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_62789540/article/details/122500983">https://blog.csdn.net/qq_62789540/article/details/122500983</a><br>
  【8】<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_58667126/article/details/126105955">https://blog.csdn.net/weixin_58667126/article/details/126105955</a><br>
  【9】<a target="_blank" rel="noopener" href="https://www.cnblogs.com/surpassme/p/16552633.html">https://www.cnblogs.com/surpassme/p/16552633.html</a><br>
  【10】<a target="_blank" rel="noopener" href="https://www.cnblogs.com/yxm-yxwz/p/16260797.html">https://www.cnblogs.com/yxm-yxwz/p/16260797.html</a><br>
  【11】<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_54542209/article/details/123282142">https://blog.csdn.net/weixin_54542209/article/details/123282142</a><br>
  【12】<a target="_blank" rel="noopener" href="http://blog.csdn.net/luxideyao/article/details/77802389">http://blog.csdn.net/luxideyao/article/details/77802389</a><br>
  【13】<a target="_blank" rel="noopener" href="https://achang.blog.csdn.net/article/details/122884222">https://achang.blog.csdn.net/article/details/122884222</a><br>
  【14】<a target="_blank" rel="noopener" href="https://www.bbsmax.com/A/gGdXBNBpJ4/">https://www.bbsmax.com/A/gGdXBNBpJ4/</a><br>
  【15】<a target="_blank" rel="noopener" href="https://www.cnblogs.com/huskysir/p/12425197.html">https://www.cnblogs.com/huskysir/p/12425197.html</a><br>
  【16】<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27645452">https://zhuanlan.zhihu.com/p/27645452</a><br>
  【17】<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/533266670">https://zhuanlan.zhihu.com/p/533266670</a><br>
  【18】<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_39314932/article/details/99338957">https://blog.csdn.net/qq_39314932/article/details/99338957</a><br>
  【19】<a target="_blank" rel="noopener" href="https://www.bbsmax.com/A/A7zgADgP54/">https://www.bbsmax.com/A/A7zgADgP54/</a><br>
  【20】<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_44690947/article/details/126236736">https://blog.csdn.net/qq_44690947/article/details/126236736</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://www.program-park.top/">大Null</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://www.program-park.top/2023/04/13/reptile_2/">https://www.program-park.top/2023/04/13/reptile_2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">此文章版权归 <a href=https://www.program-park.top/>程序园</a> 所有，如有转载，请注明来自原作者。</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></div><div class="post_share"><div class="social-share" data-image="/img/reptile/1.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/04/14/python_16/" title="Python 脚本之对目录下所有文件进行字符串替换"><img class="cover" src="/img/python/1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Python 脚本之对目录下所有文件进行字符串替换</div></div></a></div><div class="next-post pull-right"><a href="/2022/08/30/reptile_1/" title="Python 爬虫基础之 urllib 库的深入使用详解"><img class="cover" src="/img/reptile/1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Python 爬虫基础之 urllib 库的深入使用详解</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/08/30/reptile_1/" title="Python 爬虫基础之 urllib 库的深入使用详解"><img class="cover" src="/img/reptile/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-30</div><div class="title">Python 爬虫基础之 urllib 库的深入使用详解</div></div></a></div><div><a href="/2021/03/12/python_1/" title="Python 脚本之将 logstash 数据按天保存在本地服务器并加密压缩"><img class="cover" src="/img/python/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-12</div><div class="title">Python 脚本之将 logstash 数据按天保存在本地服务器并加密压缩</div></div></a></div><div><a href="/2021/08/09/python_10/" title="Windows 下安装 Python 环境"><img class="cover" src="/img/python/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-09</div><div class="title">Windows 下安装 Python 环境</div></div></a></div><div><a href="/2021/08/10/python_11/" title="Python 脚本之对文件进行哈希校验"><img class="cover" src="/img/python/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-10</div><div class="title">Python 脚本之对文件进行哈希校验</div></div></a></div><div><a href="/2021/08/13/python_12/" title="AttributeEroor: ‘list‘ object has no attribute ‘clear‘"><img class="cover" src="/img/python/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-13</div><div class="title">AttributeEroor: ‘list‘ object has no attribute ‘clear‘</div></div></a></div><div><a href="/2021/08/23/python_13/" title="python整数相除保留小数SyntaxError: from __future__ imports must occur at the beginning of the file"><img class="cover" src="/img/python/1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-23</div><div class="title">python整数相除保留小数SyntaxError: from __future__ imports must occur at the beginning of the file</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatat_img.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">大Null</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">206</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">37</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/program-park"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/program-park" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:lkm869666@126.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.csdn.net/weixin_44758876" target="_blank" title="CSDN"><i class="fa-solid fa-c"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">网站正在优化中......</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E5%89%8D%E8%A8%80"><span class="toc-text">1. 前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Xpath"><span class="toc-text">2. Xpath</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E6%8F%92%E4%BB%B6-%E5%BA%93%E5%AE%89%E8%A3%85"><span class="toc-text">2.1 插件&#x2F;库安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8"><span class="toc-text">2.2 基础使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-Xpath%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-text">2.3 Xpath表达式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA"><span class="toc-text">2.4 案例演示</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-1-%E6%9F%90%E5%BA%A6%E7%BD%91%E7%AB%99%E6%A1%88%E4%BE%8B"><span class="toc-text">2.4.1 某度网站案例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-JsonPath"><span class="toc-text">3. JsonPath</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E5%BA%93%E5%AE%89%E8%A3%85"><span class="toc-text">3.1 库安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8"><span class="toc-text">3.2 基础使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-JsonPath%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-text">3.2 JsonPath表达式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA"><span class="toc-text">3.3 案例演示</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-BeautifulSoup"><span class="toc-text">4. BeautifulSoup</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E5%BA%93%E5%AE%89%E8%A3%85"><span class="toc-text">4.1 库安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8"><span class="toc-text">4.2 基础使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E5%B8%B8%E8%A7%81%E6%96%B9%E6%B3%95"><span class="toc-text">4.3 常见方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA"><span class="toc-text">4.4 案例演示</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-text">参考文献</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/06/07/python_19/" title="Python 脚本之获取 Splunk 数据保存到本地文件"><img src="/img/python/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python 脚本之获取 Splunk 数据保存到本地文件"/></a><div class="content"><a class="title" href="/2023/06/07/python_19/" title="Python 脚本之获取 Splunk 数据保存到本地文件">Python 脚本之获取 Splunk 数据保存到本地文件</a><time datetime="2023-06-07T04:23:29.000Z" title="发表于 2023-06-07 12:23:29">2023-06-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/17/linux_7/" title="VMware 虚拟机安装 OpenEuler 欧拉系统"><img src="/img/linux/3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="VMware 虚拟机安装 OpenEuler 欧拉系统"/></a><div class="content"><a class="title" href="/2023/05/17/linux_7/" title="VMware 虚拟机安装 OpenEuler 欧拉系统">VMware 虚拟机安装 OpenEuler 欧拉系统</a><time datetime="2023-05-17T08:30:44.000Z" title="发表于 2023-05-17 16:30:44">2023-05-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/17/python_18/" title="Python 脚本之给图片批量添加水印"><img src="/img/python/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python 脚本之给图片批量添加水印"/></a><div class="content"><a class="title" href="/2023/05/17/python_18/" title="Python 脚本之给图片批量添加水印">Python 脚本之给图片批量添加水印</a><time datetime="2023-05-17T06:56:21.000Z" title="发表于 2023-05-17 14:56:21">2023-05-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/09/python_17/" title="Python 脚本之获取 Splunk 数据发送到 Kafka"><img src="/img/python/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python 脚本之获取 Splunk 数据发送到 Kafka"/></a><div class="content"><a class="title" href="/2023/05/09/python_17/" title="Python 脚本之获取 Splunk 数据发送到 Kafka">Python 脚本之获取 Splunk 数据发送到 Kafka</a><time datetime="2023-05-09T08:03:08.000Z" title="发表于 2023-05-09 16:03:08">2023-05-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/14/python_16/" title="Python 脚本之对目录下所有文件进行字符串替换"><img src="/img/python/1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python 脚本之对目录下所有文件进行字符串替换"/></a><div class="content"><a class="title" href="/2023/04/14/python_16/" title="Python 脚本之对目录下所有文件进行字符串替换">Python 脚本之对目录下所有文件进行字符串替换</a><time datetime="2023-04-14T01:55:01.000Z" title="发表于 2023-04-14 09:55:01">2023-04-14</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/9.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By 大Null</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'https://www.program-park.top/2023/04/13/reptile_2/'
    this.page.identifier = '/2023/04/13/reptile_2/'
    this.page.title = 'Python 爬虫之如何对爬取到的数据进行解析'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }

  document.getElementById('darkmode').addEventListener('click', () => {
    setTimeout(() => window.disqusReset(), 200)
  })
}

if ('Valine' === 'Disqus' || !false) {
  if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>